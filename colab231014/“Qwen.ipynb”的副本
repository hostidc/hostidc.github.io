{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Xi1tvllKKJiIsK2EnPx5aCACkzHKSKDU","timestamp":1694741766823}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sR1QlqByxZJx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c1b397aa-169d-4cb9-9b57-09bddf3772f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","fatal: destination path 'flash-attention' already exists and is not an empty directory.\n","/content/flash-attention\n","Processing /content/flash-attention\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==1.0.8) (2.0.1+cu118)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==1.0.8) (0.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn==1.0.8) (23.1)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn==1.0.8) (1.11.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==1.0.8) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==1.0.8) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==1.0.8) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==1.0.8) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==1.0.8) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==1.0.8) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn==1.0.8) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn==1.0.8) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==1.0.8) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn==1.0.8) (1.3.0)\n","Building wheels for collected packages: flash-attn\n"]}],"source":["# 通义千问7B 一键部署\n","# 本脚本基于colab测试通过，因为大部分用户是免费用户，只能分到T4显卡，因此模型做了int8量化。\n","\n","!git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention\n","%cd flash-attention && pip install .\n","%cd ..\n","\n","!git clone https://github.com/fishfl/Qwen-7B.git\n","%cd Qwen-7B\n","!pip install -r requirements.txt\n","!pip install -r requirements_web_demo.txt\n","!pip install bitsandbytes\n","!python web_demo.py --share\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Pb7j1lJBjV7V"},"execution_count":null,"outputs":[]}]}